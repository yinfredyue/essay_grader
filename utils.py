import re
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

NLTK_DIR = "./nltk_data"

nltk.data.path.append(NLTK_DIR)
nltk.download('punkt', quiet=True, download_dir=NLTK_DIR)
nltk.download('averaged_perceptron_tagger', quiet=True, download_dir=NLTK_DIR)

porter_stemmer = PorterStemmer()


# Reference: https://stackoverflow.com/a/31505798/9057530
alphabets = "([A-Za-z])"
prefixes = "(Mr|St|Mrs|Ms|Dr)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](com|net|org|io|gov)"
digits = "([0-9])"


def split_into_sentences(text):
    text = " " + text + "  "
    text = text.replace("\n", " ")
    text = re.sub(prefixes, "\\1<prd>", text)
    text = re.sub(websites, "<prd>\\1", text)
    text = re.sub(digits + "[.]" + digits, "\\1<prd>\\2", text)
    if "..." in text:
        text = text.replace("...", "<prd><prd><prd>")
    if "Ph.D" in text:
        text = text.replace("Ph.D.", "Ph<prd>D<prd>")
    text = re.sub("\s" + alphabets + "[.] ", " \\1<prd> ", text)
    text = re.sub(acronyms+" "+starters, "\\1<stop> \\2", text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]", "\\1<prd>\\2<prd>\\3<prd>", text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]", "\\1<prd>\\2<prd>", text)
    text = re.sub(" "+suffixes+"[.] "+starters, " \\1<stop> \\2", text)
    text = re.sub(" "+suffixes+"[.]", " \\1<prd>", text)
    text = re.sub(" " + alphabets + "[.]", " \\1<prd>", text)
    if "”" in text:
        text = text.replace(".”", "”.")
    if "\"" in text:
        text = text.replace(".\"", "\".")
    if "!" in text:
        text = text.replace("!\"", "\"!")
    if "?" in text:
        text = text.replace("?\"", "\"?")
    text = text.replace(".", ".<stop>")
    text = text.replace("?", "?<stop>")
    text = text.replace("!", "!<stop>")
    text = text.replace("<prd>", ".")
    sentences = text.split("<stop>")
    sentences = sentences[:-1]
    sentences = [s.strip() for s in sentences]
    return sentences


def split_into_words(txt, use_nltk):
    # NLTK tokenizer is supposed to be more accurate.
    # But MS Word count words by whitespace.
    if use_nltk:
        return nltk.word_tokenize(txt)
    else:
        return txt.split()

def pos_tag(txt):
    tokens = nltk.word_tokenize(txt)
    tagged_tokens = nltk.pos_tag(tokens)

    return tagged_tokens

def stem(txt):
    res = []

    tokens = word_tokenize(txt)
    for token in tokens:
        res.append(porter_stemmer.stem(token))

    return " ".join(res)




