Throughout my previous research journey, I have been thinking about two questions: Why do we need more modalities? And how do we understand the relationship between different sensory modalities?  

With these questions in mind, I started my first project at Stanford Vision and Learning Lab with Prof. Fei-Fei Li and Prof. Jiajun Wu. This paper was published in the 6th Conference on Robot Learning, and I am the lead first author of this paper [1]. This project is inspired by human perception systems. Humans use all our senses to accomplish different tasks in daily activities. Colors, patterns, and structures are perceived through vision. Sounds and vibrations are captured through listening. Local geometry and materials can be sensed by our skin, especially our fingers. Each modality contains tremendous amounts of information to help us in task completion. Similarly, we should be able to endow robot agents with this kind of capability. However, existing work on robotic manipulation mostly relies on one, or occasionally two modalities, such as vision and touch. We are the first to build a complete perception system that includes all three senses. We first equip the robot with a third-view camera and a contact microphone. Then, we collaborated with Prof. Ted Adelson to put a GelSight sensor on one of the robot fingers to enable the touch sense. We proposed a self-attention mechanism to fuse all three senses and let the robot finish a dense-packing task and a pouring task. Based on our ablation study on multi-modal and analysis on the attention score of each modality, we find that the success rate increases when there are more modalities, and they are complementing each other in different tasks. For example, in the dense-packing tasks, the robot grasps an object and tries to put it in an empty space in the box. Vision dominates in the aligning phase to help locate where the box is. Then, when collision/sliding happens, the attention score of audio and tactile suddenly increases and complements vision during the entire contact process. Acoustic data from the contact microphone sends immediate alerts when collision and sliding happen, while tactile data from the GelSight sensor provides precise local geometry of objects that reveal their status. Another example is in the pouring task, while in this task, acoustic data plays a significant role in determining when to stop pouring. These findings align with the way of human perception in task completion.   

During my first project, I found that the characteristics of each modality are very task specific. Besides manipulation tasks, I am also interested in multi-modal learning in navigation tasks. Therefore, I started my second project. This paper was submitted to IEEE International Conference on Robotics and Automation and is under review [2]. I am one of the lead first authors of this paper. This project is about embodied AI simulation and is inspired by how humans navigate in household environments. For example, if you hear a dishwashing sound, you would be able to locate the kitchen and navigate to the sound source. However, most simulators assume vision-only agents. Existing simulators with sound simulation such as SoundSpace 1.0/2.0 [4, 5] and ThreeDWorld [6] have their distinct features as well as limitations. We developed a simulator called Sonicverse that can both render continuous audio in 3D spaces in real-time and achieve high realism by using the complete scene geometry and surface material properties. For semantic audio-visual navigation, we proposed a new multi-task learning model that achieves state-of-the-art performance. In addition, we are the first to show that agents trained in a simulator can be successfully deployed in real-world environments for audio-visual navigation.   

My first two projects are robot/agent centric. I started my third project from a different point of view to understand multi-modal: object-centric learning. This paper was submitted to IEEE/CVF Conference on Computer Vision and Pattern Recognition and is under review [3]. I am one of the lead first authors of this paper, and its inspiration is from how humans perceive objects in the real world. Objects exist not only as visual entities, but also can be touched and make sounds during interactions. Therefore, we introduced a dataset that contains multisensory information of 1000 neural objects and 100 real-world household objects, including visual, acoustic, and tactile data, with a benchmark suite that has 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation. The datasets and tasks can help us interpret the relationship between modalities. Cross-sensory retrieval, for instance, is especially important in understanding the correspondence of different modalities. We proposed baseline methods and metrics for each task, and the results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch in each task.   

As I mentioned before, my research has not fully answered my question about multimodality. For my future work, I’m planning to work on two directions: algorithm and hardware design. Besides continuing my current work with Prof. Jiajun Wu and Prof. Jeannette Bohg, I’m also interested in working with Prof. Mykel J. Kochenderfer because my interest aligns well with his focus on multi-modal sensing and navigation. When I was taking AA228, Prof. Kochenderfer presented Augmented Cane [7], which is a white cane with a comprehensive set of sensors and intuitive feedback method to steer people with impaired vision. I was fascinated by the design, and I wonder if audio might also be useful to help navigation, especially in indoor environments. In my work [2], I added audio simulation to enable the household agent to both see and hear. Audio contains semantic information that helps locate the sound source and provides strong directional signal to help navigate to the target. I hope to implement a multisensory system for Augmented Cane that facilitates a more comprehensive understanding and a safer navigation of the surrounding environment.

Besides the Computer Science department, I also seek for opportunity to work with Prof. Mark Cutkosky from the Mechanical Engineering department because of my interest in tactile sensing which I used in all my manipulation tasks. However, the current GelSight sensor I’m using has many limitations. For instance, it captures the deformation of the elastomer with a camera behind it, so that it can only capture high dimensional image data, which might be redundant in some tasks. The contact forces are inferred from changes in images instead of direct measurement, which is around 30 Hz. Compared to FBG-based tactile sensor [8], it is not robust, and its force measurement is not accurate enough. Therefore, I would like to contribute to the development of tactile sensors in my future work. 

As for the aspect outside academic world, the inspiration of my journey in studying multi-modal and robot learning actually comes from my domestic partner. My partner and I cooperate every day in facing little challenges caused by Attention-Deficit/Hyperactivity Disorder. In conversations, she will be distracted by any small incidents happening around her and struggles in simply finishing reading the assignment requirement without skipping random text. Her attention usually cannot last long for solely listening or watching, however, having software that reads out the screen, close captioning of videos, or sensory stickers in hand help her be much more concentrated. This is only one of the numerous tasks we rely on multisensory in our daily life. 

As a PhD student, I wish to continue my research on multi-modal and robot learning at Stanford for this diverse and innovative scientific community. For my future career plan, I hope to have an opportunity to study robotics as my life-long goal and am thrilled to contribute to building intelligent multisensory robots that can benefit our society and help people with various needs. I am confident that I can be a competent researcher at Stanford, equipped with professional knowledge, outstanding research abilities as well as a constant interest in multi-modal and robot learning. 
